{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.random.set_seed(777) # for reproductibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2,1,1],\n",
    "         [2,1,3,2],\n",
    "         [3,1,3,4],\n",
    "         [4,1,5,5],\n",
    "         [1,7,5,5],\n",
    "         [1,2,5,6],\n",
    "         [1,6,6,6],\n",
    "         [1,7,7,7]]\n",
    "y_data = [[0,0,1],\n",
    "         [0,0,1],\n",
    "         [0,0,1],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [1,0,0],\n",
    "         [1,0,0]]\n",
    "\n",
    "# convert into numpy and float format\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 3)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 3 # class의 갯수\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-1.4564867 ,  0.53983474, -1.1366715 ],\n",
      "       [ 0.51545775, -0.44776005, -0.72691107],\n",
      "       [-0.44219774, -1.5832956 , -0.31117675],\n",
      "       [-0.46808162,  1.0022584 , -0.55059594]], dtype=float32)> <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([-0.19601157,  0.21892233, -0.69331926], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "variables = [W,b]\n",
    "\n",
    "print(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3.0027157e-01 6.7773020e-01 2.1998271e-02]\n",
      " [4.8193250e-02 9.3165678e-01 2.0150051e-02]\n",
      " [3.7100344e-04 9.9944788e-01 1.8108847e-04]\n",
      " [1.1353292e-04 9.9979514e-01 9.1305468e-05]\n",
      " [9.3598300e-01 6.3850135e-02 1.6696952e-04]\n",
      " [2.6503863e-02 9.7132540e-01 2.1707639e-03]\n",
      " [8.0059057e-01 1.9889010e-01 5.1927834e-04]\n",
      " [8.8340557e-01 1.1642074e-01 1.7364482e-04]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.5902415e-09 1.0000000e+00 2.4629851e-09]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# softmax onehot test\n",
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
    "\n",
    "print(hypothesis(sample_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.4330945, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x # x^2\n",
    "dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 0.13058853,  0.6111318 , -0.7417203 ],\n",
      "       [ 0.65518653, -0.16446908, -0.49071747],\n",
      "       [ 0.40579358,  0.45677286, -0.8625665 ],\n",
      "       [ 0.40312877,  0.46166247, -0.8647913 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.12442906,  0.24488954, -0.3693186 ], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "print(grad_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.031811\n",
      "Loss at epoch 100: 0.649606\n",
      "Loss at epoch 200: 0.551993\n",
      "Loss at epoch 300: 0.493788\n",
      "Loss at epoch 400: 0.447853\n",
      "Loss at epoch 500: 0.407455\n",
      "Loss at epoch 600: 0.369700\n",
      "Loss at epoch 700: 0.332856\n",
      "Loss at epoch 800: 0.295840\n",
      "Loss at epoch 900: 0.260075\n",
      "Loss at epoch 1000: 0.239424\n",
      "Loss at epoch 1100: 0.227316\n",
      "Loss at epoch 1200: 0.216379\n",
      "Loss at epoch 1300: 0.206424\n",
      "Loss at epoch 1400: 0.197322\n",
      "Loss at epoch 1500: 0.188967\n",
      "Loss at epoch 1600: 0.181268\n",
      "Loss at epoch 1700: 0.174153\n",
      "Loss at epoch 1800: 0.167558\n",
      "Loss at epoch 1900: 0.161429\n",
      "Loss at epoch 2000: 0.155718\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X,Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' % (i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00205497 0.08289105 0.91505396]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_data = [[2,1,3,2]] # answer_label = [0,0,1]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
    "\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a)\n",
    "print(tf.argmax(a,1)) # index: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3.7569955e-06 1.1868312e-03 9.9880946e-01]\n",
      " [2.0549716e-03 8.2891054e-02 9.1505396e-01]\n",
      " [5.6705659e-08 1.6364072e-01 8.3635926e-01]\n",
      " [1.4599668e-06 8.4985423e-01 1.5014429e-01]\n",
      " [2.5336072e-01 7.3442239e-01 1.2216868e-02]\n",
      " [1.3299116e-01 8.6698902e-01 1.9764271e-05]\n",
      " [7.5716817e-01 2.4280141e-01 3.0437392e-05]\n",
      " [9.1867179e-01 8.1327625e-02 5.3354438e-07]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b = hypothesis(x_data)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1)) # matches with y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert as class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.583602\n",
      "Loss at epoch 500: 0.340002\n",
      "Loss at epoch 1000: 0.217327\n",
      "Loss at epoch 1500: 0.174352\n",
      "Loss at epoch 2000: 0.145320\n"
     ]
    }
   ],
   "source": [
    "class softmax_classifier(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifier, self).__init__()\n",
    "        self.W = tf.Variable(tf.compat.v1.random_normal((4, nb_classes)), name='weight')\n",
    "        self.b = tf.Variable(tf.compat.v1.random_normal((nb_classes,)), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def cost_fn(self, X, Y):\n",
    "        logits = self.softmax_regression(X)\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))\n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.cost_fn(x_data, y_data)\n",
    "            grads = tape.gradient(cost, self.variables)\n",
    "            return grads\n",
    "        \n",
    "    def fit(self, X, Y, epochs=2000, verbose=500):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            grads = self.grad_fn(X, Y)\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Loss at epoch %d: %f' % (i+1, self.cost_fn(X,Y).numpy()))\n",
    "                \n",
    "model = softmax_classifier(nb_classes)\n",
    "model.fit(x_data, y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
